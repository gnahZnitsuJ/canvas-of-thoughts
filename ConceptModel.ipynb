{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for preprocessing of natural language\n",
    "# important nltk requisite downloads include: tokenize, reuters\n",
    "import nltk\n",
    "# reuters articles corpus for training\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "# nengo: see www.nengo.ai\n",
    "import nengo\n",
    "import nengo_spa as spa\n",
    "import matplotlib.pyplot as plt\n",
    "import nengo_dl\n",
    "\n",
    "# gensim for seed word embedding\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# numpy\n",
    "import numpy as np\n",
    "\n",
    "# other string processing\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters; change as needed\n",
    "\n",
    "# random seeding \n",
    "seed = 42\n",
    "rng = np.random.RandomState(seed + 1) # 43 no reason in particular\n",
    "\n",
    "# dimension and max similarity of semantic pointer vocabulary\n",
    "# chosen using Johnson-Lindenstrauss Lemma to represent n vectors with\n",
    "# (arbitrarily chosen) given max similarity angles and dimensions\n",
    "rep_vocab_dim = 128 # must be divisible by 16 for spa.State to work.\n",
    "rep_vocab_max_sim = 0.8 # I think this only matters for .populate() ?\n",
    "\n",
    "# context length: number of words looked at before\n",
    "context_length = 20\n",
    "\n",
    "# training impression\n",
    "tr_impression = 0.01 # an unrealistic actual impression for time's sake\n",
    "# dataset restrictions, number of articles in reuters corpus\n",
    "training_restriction = 20\n",
    "testing_restriction = 2\n",
    "\n",
    "# model\n",
    "model_lr = 0.005 # learning rate for PES rule.\n",
    "\n",
    "# special tokens\n",
    "unknown_token = \"CV_UNK\"\n",
    "pad_token = \"CV_PAD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing functions\n",
    "\n",
    "# vocabulary preprocessing functions to overcome limitations with\n",
    "#   nengo_spa python identifier limitations\n",
    "CharReplacements = {\n",
    "    '!': 'CV_EXCLAMATION_MARK',\n",
    "    '\"': 'CV_DOUBLE_QUOTE',\n",
    "    '#': 'CV_HASH',\n",
    "    '$': 'CV_DOLLAR_SIGN',\n",
    "    '%': 'CV_PERCENT_SIGN',\n",
    "    '&': 'CV_AMPERSAND',\n",
    "    \"'\": 'CV_SINGLE_QUOTE',\n",
    "    '(': 'CV_LEFT_PARENTHESIS',\n",
    "    ')': 'CV_RIGHT_PARENTHESIS',\n",
    "    '*': 'CV_ASTERISK',\n",
    "    '+': 'CV_PLUS',\n",
    "    ',': 'CV_COMMA',\n",
    "    '-': 'CV_HYPHEN',\n",
    "    '.': 'CV_PERIOD',\n",
    "    '/': 'CV_FORWARD_SLASH',\n",
    "    ':': 'CV_COLON',\n",
    "    ';': 'CV_SEMICOLON',\n",
    "    '<': 'CV_LESS_THAN',\n",
    "    '=': 'CV_EQUALS',\n",
    "    '>': 'CV_GREATER_THAN',\n",
    "    '?': 'CV_QUESTION_MARK',\n",
    "    '@': 'CV_AT_SYMBOL',\n",
    "    '[': 'CV_LEFT_BRACKET',\n",
    "    '\\\\': 'CV_BACKSLASH',\n",
    "    ']': 'CV_RIGHT_BRACKET',\n",
    "    '^': 'CV_CARET',\n",
    "    '_': 'CV_UNDERSCORE',\n",
    "    '`': 'CV_GRAVE_ACCENT',\n",
    "    '{': 'CV_LEFT_BRACE',\n",
    "    '|': 'CV_PIPE',\n",
    "    '}': 'CV_RIGHT_BRACE',\n",
    "    '~': 'CV_TILDE',\n",
    "}\n",
    "CharReplacementsTable = str.maketrans(CharReplacements) # translation table\n",
    "\n",
    "InvCharReplacements = {v:k for k, v in CharReplacements.items()} # inverse dict for translation\n",
    "\n",
    "# turning \"words\" (tokens) to usable identifiers\n",
    "def WordsToSPAVocab(w: list):\n",
    "    # translate special characters capital start for identifiers\n",
    "    w = [x if x[0:3] == \"CV_\" else \"WV_\" + x.translate(CharReplacementsTable) for x in w]\n",
    "    return w\n",
    "\n",
    "# turning usable identifiers into \"words\" (tokens)\n",
    "def SPAVocabToWords(w: list):\n",
    "    \n",
    "    # Create a function to replace all the placeholders with their corresponding characters\n",
    "    def replace_placeholder_with_char(match):\n",
    "        word = match.group(0)\n",
    "        return InvCharReplacements.get(word, word)  # Return the original if no replacement exists\n",
    "\n",
    "    # Regular expression to match all the placeholder words (e.g., \"EXCLAMATION_MARK\")\n",
    "    pattern = r'\\b(?:' + '|'.join(map(re.escape, InvCharReplacements.keys())) + r')\\b'\n",
    "\n",
    "    # inverse operations for WordsToSPAVocab\n",
    "    w = [x[3:] for x in w]\n",
    "    w = [re.sub(pattern, replace_placeholder_with_char, x) for x in w] # removing special characters\n",
    "    return w "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing\n",
    "\n",
    "# distinguishing training and testing data\n",
    "reuters_training_ids = [x for x in reuters.fileids() if \"training/\" in x]\n",
    "reuters_testing_ids = [x for x in reuters.fileids() if \"test/\" in x]\n",
    "\n",
    "# restriction of training set for time's sake\n",
    "reuters_training_ids = reuters_training_ids[:training_restriction]\n",
    "reuters_testing_ids = reuters_testing_ids[:testing_restriction]\n",
    "\n",
    "# train and validation\n",
    "\n",
    "# training set vocabulary\n",
    "vocab = [\n",
    "    t\n",
    "    for x in reuters_training_ids\n",
    "    for t in reuters.words(x)\n",
    "] # all words in training\n",
    "# print(len(vocab))\n",
    "vocab = list(set(vocab)) # unique words\n",
    "# vocab += [unknown_token] # unknown placeholder (in strict case)\n",
    "vocab += [pad_token] # padding character\n",
    "# print(len(vocab))\n",
    "\n",
    "spa_vocab = WordsToSPAVocab(vocab)\n",
    "\n",
    "# training set generation\n",
    "training_set = [\n",
    "    [reuters.words(x)[max(0,i-context_length):max(0,i)], [reuters.words(x)[i]]]\n",
    "    for x in reuters_training_ids\n",
    "    for i in range(len(reuters.words(x)))\n",
    "]\n",
    "# padding for training set\n",
    "training_set = [\n",
    "    [[pad_token] * (context_length - len(x[0])) + x[0], x[1]]\n",
    "    for x in training_set\n",
    "]\n",
    "\n",
    "# testing set generation\n",
    "testing_set = [\n",
    "    [reuters.words(x)[max(0,i-context_length):max(0,i)], [reuters.words(x)[i]]]\n",
    "    for x in reuters_testing_ids\n",
    "    for i in range(len(reuters.words(x)))\n",
    "]\n",
    "# padding for testing set\n",
    "testing_set = [\n",
    "    [[pad_token] * (context_length - len(x[0])) + x[0], x[1]]\n",
    "    for x in testing_set\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semantic pointer vocabulary generation for model\n",
    "\n",
    "# initial generation\n",
    "\n",
    "# attempt at adding a basic \"seed\" word embedding in here\n",
    "# not sure how useful it is to have this learned further\n",
    "seed_vocab_data = [WordsToSPAVocab(i) for x in reuters_training_ids for i in reuters.sents(x)]\n",
    "seed_vocab_model = Word2Vec(sentences=seed_vocab_data, min_count=1, vector_size=rep_vocab_dim, window = context_length, epochs = 50)\n",
    "\n",
    "# Seed vocab data\n",
    "# seed_vocab_vectors = {i: seed_vocab_model.wv.get_vector(i) for i in spa_vocab[0:-2]} # removing pad_token and unknown_token\n",
    "seed_vocab_vectors = {i: seed_vocab_model.wv.get_vector(i) for i in spa_vocab[0:-1]} # removing pad_token\n",
    "\n",
    "# vocabulary for our model: store of semantic pointers \n",
    "model_vocab = spa.Vocabulary(dimensions=rep_vocab_dim, strict=False, pointer_gen=None, max_similarity=rep_vocab_max_sim)\n",
    "# for random creation, model_vocab.populate(\";\".join(spa_vocab))\n",
    "# for random normalized creation, model_vocab.populate(\".normalized();\".join(spa_vocab))\n",
    "# for random unitary creation, model_vocab.populate(\".unitary();\".join(spa_vocab))\n",
    "\n",
    "# creating pointers\n",
    "for i,j in seed_vocab_vectors.items():\n",
    "    model_vocab.add(key = i, p = j)\n",
    "# padding and unknown characters\n",
    "# model_vocab.populate(\";\".join([pad_token, unknown_token])) # strict case\n",
    "model_vocab.add(key = pad_token, p = np.zeros(rep_vocab_dim))\n",
    "\n",
    "\n",
    "# input\n",
    "# custom input\n",
    "# s1 = \"This is a sentence.\"\n",
    "# nltk.word_tokenize(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input \n",
    "\n",
    "# model input functions\n",
    "# will return a context vector of tokens dependent on time\n",
    "def context_in(t):\n",
    "\tif t <= training_time:\n",
    "\t\timpression = tr_impression # how long the model sees each word for (impression time)\n",
    "\t\tpair = training_set[int(t // impression)]\n",
    "\t\tleft = WordsToSPAVocab(pair[0])\n",
    "\t\treturn \" + \".join(left) # returns training context\n",
    "\telse:\n",
    "\t\tt_test = t - training_time\n",
    "\t\timpression = tr_impression # how long the model sees each word for (impression time)\n",
    "\t\tpair = testing_set[int(t_test // impression)]\n",
    "\t\t# left = WordsToSPAVocab([unknown_token if x not in vocab else x for x in pair[0]]) # strict case\n",
    "\t\tleft = WordsToSPAVocab(pair[0])\n",
    "\t\treturn \" + \".join(left) # returns training context\n",
    "\t\n",
    "\n",
    "# will return the desired predicted token dependent on time\n",
    "def find_target(t):\n",
    "\tif t <= training_time:\n",
    "\t\timpression = tr_impression # how long the model sees each word for (impression time)\n",
    "\t\tpair = training_set[int(t // impression)]\n",
    "\t\treturn WordsToSPAVocab(pair[1])[0] # should find targets in parallel to context\n",
    "\telse:\n",
    "\t\tt_test = t - training_time\n",
    "\t\timpression = tr_impression # how long the model sees each word for (impression time)\n",
    "\t\tpair = testing_set[int(t_test // impression)]\n",
    "\t\t# should find targets in parallel to context (strict case)\n",
    "\t\t# return WordsToSPAVocab(unknown_token if x not in vocab else x for x in pair[1])[0] \n",
    "\t\treturn WordsToSPAVocab(pair[1])[0]\n",
    "\n",
    "# training time length\n",
    "training_time = len(training_set)*tr_impression\n",
    "# testing time length\n",
    "testing_time = (len(testing_set)-1)*tr_impression\n",
    "\n",
    "def is_recall(t):\n",
    "\treturn t > training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "with spa.Network(seed=seed) as model:\n",
    "    # transcoding training into semantic pointers\n",
    "    context = spa.Transcode(context_in, output_vocab=model_vocab)\n",
    "    target = spa.Transcode(find_target, output_vocab=model_vocab)\n",
    "\n",
    "    # State (ensembles) for learning\n",
    "    pre_state = spa.State(\n",
    "        model_vocab, subdimensions=model_vocab.dimensions, represent_cc_identity=False\n",
    "    )\n",
    "    post_state = spa.State(\n",
    "        model_vocab, subdimensions=model_vocab.dimensions, represent_cc_identity=False\n",
    "    )\n",
    "    error = spa.State(model_vocab)\n",
    "\n",
    "    # signal connections between objects see report for connection logic\n",
    "    # input and error\n",
    "    context >> pre_state\n",
    "    -post_state >> error\n",
    "    target >> error\n",
    "\n",
    "    # learning between ensembles\n",
    "    assert len(pre_state.all_ensembles) == 1\n",
    "    assert len(post_state.all_ensembles) == 1\n",
    "    learning_connection = nengo.Connection(\n",
    "        pre_state.all_ensembles[0],\n",
    "        post_state.all_ensembles[0],\n",
    "        function=lambda x: np.random.random(model_vocab.dimensions),\n",
    "        learning_rule_type=nengo.PES(model_lr), # Prescribed Error Sensitivity\n",
    "    )\n",
    "    nengo.Connection(error.output, learning_connection.learning_rule, transform=-1)\n",
    "\n",
    "    # Suppress learning in the final iteration to test\n",
    "    is_recall_node = nengo.Node(is_recall, size_out=1)\n",
    "    for ens in error.all_ensembles:\n",
    "        nengo.Connection(\n",
    "            is_recall_node, ens.neurons, transform=-100 * np.ones((ens.n_neurons, 1))\n",
    "        )\n",
    "\n",
    "    # Probes to record simulation data\n",
    "    p_target = nengo.Probe(target.output)\n",
    "    p_error = nengo.Probe(error.output, synapse=0.01)\n",
    "    p_post_state = nengo.Probe(post_state.output, synapse=0.01)\n",
    "    # sampling more consistently for word data\n",
    "    p_target_word = nengo.Probe(target.output, sample_every=tr_impression/2)\n",
    "    p_result_word = nengo.Probe(post_state.output, synapse=0.01, sample_every=tr_impression/2)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preliminary simulation\n",
    "simulation_length = training_time + testing_time\n",
    "\n",
    "with nengo_dl.Simulator(model) as sim:\n",
    "    sim.run(simulation_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Amount of simulated training time: {training_time}\")\n",
    "print(f\"Amount of simulated testing time: {testing_time}\")\n",
    "\n",
    "# plotting metrics for training\n",
    "# plt.figure()\n",
    "# plt.plot(sim.trange(), np.linalg.norm(sim.data[p_error], axis=1))\n",
    "# plt.xlim(0, simulation_length)\n",
    "# plt.ylim(bottom=0)\n",
    "# plt.title(\"Error signal\")\n",
    "# plt.xlabel(\"Time [s]\")\n",
    "# plt.ylabel(\"Error norm\")\n",
    "\n",
    "# plotting metrics for testing\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(sim.trange(), np.linalg.norm(spa.similarity(sim.data[p_post_state], model_vocab) - spa.similarity(sim.data[p_target], model_vocab), axis = 1))\n",
    "plt.ylim(bottom=0)\n",
    "plt.title(\"Training\")\n",
    "plt.ylabel(\"Norm of Difference in Vocab Similarity between Result and Target\")\n",
    "plt.xlabel(f\"Time (Testing Starts After: {training_time})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \".join(SPAVocabToWords([\"WV_\" + spa.text(sim.data[p_result_word][i], vocab=model_vocab, maximum_count=1).split(\"WV_\",1)[-1] \n",
    " for i in range(len(training_set), len(training_set)+100)])[::2]))\n",
    "print(\" \".join(SPAVocabToWords([\"WV_\" + spa.text(sim.data[p_target_word][i], vocab=model_vocab, maximum_count=1).split(\"WV_\",1)[-1] \n",
    " for i in range(len(training_set), len(training_set)+100)])[::2]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
